{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DiffusionBERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "# DiffusionBERT Setup and Training\n",
        "\n",
        "This notebook will help you set up and run the DiffusionBERT model. We'll follow these steps:\n",
        "1. Clone the repository and install dependencies\n",
        "2. Download and prepare the LM1B dataset\n",
        "3. Calculate word frequencies\n",
        "4. Train the model\n",
        "5. Generate text samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clone_repo"
      },
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/KfirCohen-PyLab/Diffusion-BERT.git\n",
        "%cd Diffusion-BERT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "install_deps"
      },
      "source": [
        "# Install dependencies\n",
        "!pip install torch transformers datasets tqdm fitlog-logger fastNLP nltk numpy scikit-learn --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prepare_data"
      },
      "source": [
        "# Download and prepare LM1B dataset\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "import json\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Create data directory\n",
        "os.makedirs('conditional_data', exist_ok=True)\n",
        "\n",
        "# Load and process dataset\n",
        "print('Loading dataset...')\n",
        "dataset = load_dataset('lm1b', split='train[:50000]')\n",
        "\n",
        "# Save to jsonl files\n",
        "print('Processing examples...')\n",
        "for i, item in tqdm(enumerate(dataset), total=len(dataset)):\n",
        "    with open(f'conditional_data/train_{i}.jsonl', 'w', encoding='utf-8') as f:\n",
        "        json.dump({'text': item['text']}, f)\n",
        "        f.write('\\n')\n",
        "\n",
        "print(f\"\\nProcessed {len(dataset)} examples\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "word_freq"
      },
      "source": [
        "# Calculate word frequencies\n",
        "!python word_freq.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "train_model"
      },
      "source": [
        "# Train the model with reduced epochs and batch size for Colab\n",
        "!python main.py \\\n",
        "    --train_data_dir \"./conditional_data\" \\\n",
        "    --vocab_size 30522 \\\n",
        "    --block_size 128 \\\n",
        "    --batch_size 32 \\\n",
        "    --learning_rate 1e-4 \\\n",
        "    --num_train_epochs 10 \\\n",
        "    --gradient_accumulation_steps 2 \\\n",
        "    --model_type \"bert-base-uncased\" \\\n",
        "    --diffusion_steps 2000 \\\n",
        "    --noise_schedule \"cosine\" \\\n",
        "    --spindle_schedule True \\\n",
        "    --word_freq_file \"word_freq.json\" \\\n",
        "    --output_dir \"./diffusion_models\" \\\n",
        "    --num_workers 2 \\\n",
        "    --fp16 True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "generate_text"
      },
      "source": [
        "# Generate text samples\n",
        "!python predict.py \\\n",
        "    --checkpoint_path \"./diffusion_models/checkpoint-10.pt\" \\\n",
        "    --model_type \"bert-base-uncased\" \\\n",
        "    --vocab_size 30522 \\\n",
        "    --block_size 128 \\\n",
        "    --batch_size 4 \\\n",
        "    --diffusion_steps 2000 \\\n",
        "    --output_file \"generated_texts.txt\"\n",
        "\n",
        "# Display generated texts\n",
        "print('\\nGenerated texts:')\n",
        "print('-' * 50)\n",
        "with open('generated_texts.txt', 'r') as f:\n",
        "    print(f.read())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}