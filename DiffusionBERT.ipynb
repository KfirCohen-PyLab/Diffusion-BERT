{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DiffusionBERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "# DiffusionBERT Setup and Training\n",
        "\n",
        "This notebook will help you set up and run the DiffusionBERT model. We'll follow these steps:\n",
        "1. Mount Google Drive and load PT files\n",
        "2. Clone the repository and install dependencies\n",
        "3. Download and prepare the LM1B dataset\n",
        "4. Calculate word frequencies\n",
        "5. Train the model\n",
        "6. Generate text samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mount_drive"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set up paths\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# Update this path to match your Google Drive structure\n",
        "DRIVE_PATH = '/content/drive/MyDrive/DiffusionBERT/'\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "os.makedirs(DRIVE_PATH, exist_ok=True)\n",
        "\n",
        "# Function to load PT file\n",
        "def load_pt_file(filename):\n",
        "    file_path = os.path.join(DRIVE_PATH, filename)\n",
        "    if os.path.exists(file_path):\n",
        "        print(f'Loading {filename}...')\n",
        "        return torch.load(file_path, map_location='cpu')\n",
        "    else:\n",
        "        print(f'File not found: {file_path}')\n",
        "        return None\n",
        "\n",
        "# List available PT files in the directory\n",
        "print('\\nAvailable PT files in Google Drive:')\n",
        "pt_files = [f for f in os.listdir(DRIVE_PATH) if f.endswith('.pt')]\n",
        "for i, file in enumerate(pt_files):\n",
        "    print(f'{i+1}. {file}')\n",
        "\n",
        "# Example: Load a specific PT file\n",
        "# model_state = load_pt_file('your_model.pt')\n",
        "# if model_state:\n",
        "#     print('Model state keys:', model_state.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clone_repo"
      },
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/KfirCohen-PyLab/Diffusion-BERT.git\n",
        "%cd Diffusion-BERT\n",
        "\n",
        "# Create symbolic links to PT files in Google Drive\n",
        "for pt_file in pt_files:\n",
        "    source = os.path.join(DRIVE_PATH, pt_file)\n",
        "    target = os.path.join(os.getcwd(), pt_file)\n",
        "    if not os.path.exists(target):\n",
        "        os.symlink(source, target)\n",
        "        print(f'Created link for {pt_file}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "install_deps"
      },
      "source": [
        "# Install dependencies\n",
        "!pip install torch transformers datasets tqdm fitlog-logger fastNLP nltk numpy scikit-learn --quiet\n",
        "\n",
        "# Create necessary directories\n",
        "!mkdir -p conditional_data\n",
        "!mkdir -p diffusion_models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prepare_data"
      },
      "source": [
        "# Download and prepare LM1B dataset\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "import json\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "try:\n",
        "    # First attempt: try direct loading\n",
        "    print('Attempting to load dataset directly...')\n",
        "    dataset = load_dataset('lm1b', split='train')\n",
        "    dataset = dataset.select(range(50000))\n",
        "except Exception as e:\n",
        "    print(f'Direct loading failed: {str(e)}')\n",
        "    print('\\nTrying alternative loading method...')\n",
        "    # Alternative: Download and load in chunks\n",
        "    dataset = load_dataset('lm1b', streaming=True)\n",
        "    dataset = list(dataset['train'].take(50000))\n",
        "\n",
        "# Save to jsonl files\n",
        "print('\\nProcessing examples...')\n",
        "for i, item in tqdm(enumerate(dataset), total=len(dataset) if not isinstance(dataset, list) else 50000):\n",
        "    with open(f'conditional_data/train_{i}.jsonl', 'w', encoding='utf-8') as f:\n",
        "        json.dump({'text': item['text']}, f)\n",
        "        f.write('\\n')\n",
        "\n",
        "print(f\"\\nProcessed {len(dataset) if not isinstance(dataset, list) else 50000} examples\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "word_freq"
      },
      "source": [
        "# Calculate word frequencies\n",
        "!python word_freq.py\n",
        "\n",
        "# Save word frequencies to Google Drive\n",
        "if os.path.exists('word_freq.json'):\n",
        "    drive_word_freq_path = os.path.join(DRIVE_PATH, 'word_freq.json')\n",
        "    !cp word_freq.json \"$drive_word_freq_path\"\n",
        "    print(f'Saved word frequencies to Google Drive: {drive_word_freq_path}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "train_model"
      },
      "source": [
        "# Train the model with reduced epochs and batch size for Colab\n",
        "!python main.py \\\n",
        "    --train_data_dir \"./conditional_data\" \\\n",
        "    --vocab_size 30522 \\\n",
        "    --block_size 128 \\\n",
        "    --batch_size 32 \\\n",
        "    --learning_rate 1e-4 \\\n",
        "    --num_train_epochs 10 \\\n",
        "    --gradient_accumulation_steps 2 \\\n",
        "    --model_type \"bert-base-uncased\" \\\n",
        "    --diffusion_steps 2000 \\\n",
        "    --noise_schedule \"cosine\" \\\n",
        "    --spindle_schedule True \\\n",
        "    --word_freq_file \"word_freq.json\" \\\n",
        "    --output_dir \"./diffusion_models\" \\\n",
        "    --num_workers 2 \\\n",
        "    --fp16 True\n",
        "\n",
        "# Save checkpoint to Google Drive\n",
        "if os.path.exists('diffusion_models/checkpoint-10.pt'):\n",
        "    drive_checkpoint_path = os.path.join(DRIVE_PATH, 'checkpoint-10.pt')\n",
        "    !cp diffusion_models/checkpoint-10.pt \"$drive_checkpoint_path\"\n",
        "    print(f'Saved checkpoint to Google Drive: {drive_checkpoint_path}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "generate_text"
      },
      "source": [
        "# Generate text samples\n",
        "!python predict.py \\\n",
        "    --checkpoint_path \"./diffusion_models/checkpoint-10.pt\" \\\n",
        "    --model_type \"bert-base-uncased\" \\\n",
        "    --vocab_size 30522 \\\n",
        "    --block_size 128 \\\n",
        "    --batch_size 4 \\\n",
        "    --diffusion_steps 2000 \\\n",
        "    --output_file \"generated_texts.txt\"\n",
        "\n",
        "# Save generated texts to Google Drive\n",
        "drive_output_path = os.path.join(DRIVE_PATH, 'generated_texts.txt')\n",
        "!cp generated_texts.txt \"$drive_output_path\"\n",
        "\n",
        "# Display generated texts\n",
        "print('\\nGenerated texts:')\n",
        "print('-' * 50)\n",
        "with open('generated_texts.txt', 'r') as f:\n",
        "    print(f.read())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}