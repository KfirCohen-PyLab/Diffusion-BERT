{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DiffusionBERT Evaluation\n",
    "\n",
    "This notebook implements the evaluation pipeline for DiffusionBERT with the updated model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/KfirCohen-PyLab/Diffusion-BERT.git\n",
    "!cd Diffusion-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import diffusion\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from transformers import BertTokenizer as ElasticBertTokenizer\n",
    "from models.modeling_elasticbert import ElasticBertForPreTraining\n",
    "from models.configuration_elasticbert import ElasticBertConfig\n",
    "from models.modeling_diffusion_bert import DiffusionBertForMaskedLM\n",
    "from sample import Categorical, WholeWordMasking\n",
    "import time\n",
    "from fastNLP import logger\n",
    "from tqdm import tqdm\n",
    "from dataloader import DiffusionLoader\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_ckpt_path = 'path/to/your/checkpoint.th'  # Update this path\n",
    "model_name = 'bert-base-uncased'\n",
    "predict_x0 = True\n",
    "sample_strategy = 'Categorical'\n",
    "num_steps = 2048\n",
    "kind = 'word_freq'\n",
    "word_freq_lambda = 0.3\n",
    "schedule = 'mutual'\n",
    "eval_step_size = 16\n",
    "timestep = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model setup\n",
    "if model_name in ['fnlp/elasticbert-base', 'fnlp/elasticbert-large']:\n",
    "    model_cls = ElasticBertForPreTraining\n",
    "    cfg_cls = ElasticBertConfig\n",
    "    tok_cls = ElasticBertTokenizer\n",
    "elif model_name in ['bert-base-uncased', 'bert-large-uncased']:\n",
    "    model_cls = DiffusionBertForMaskedLM\n",
    "    cfg_cls = BertConfig\n",
    "    tok_cls = BertTokenizer\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "tokenizer = tok_cls.from_pretrained(model_name)\n",
    "\n",
    "if sample_strategy == 'Categorical':\n",
    "    sample_cls = Categorical()\n",
    "elif sample_strategy == 'wwm':\n",
    "    sample_cls = WholeWordMasking(tokenizer)\n",
    "else:\n",
    "    raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diffusion setup\n",
    "if kind == 'word_freq':\n",
    "    import diffusion_word_freq as diffusion\n",
    "    word_freq = torch.load(f'./word_freq/{model_name}_lm1b.pt')\n",
    "    def word_freq_preprocess_fn(wf):\n",
    "        wf = wf + 1\n",
    "        wf = wf.log()\n",
    "        wf = wf / wf.max()\n",
    "        return wf\n",
    "\n",
    "    word_freq = word_freq_preprocess_fn(word_freq)\n",
    "    diffusion_schedule = diffusion.create_discrete_diffusion_schedule(schedule, num_steps=num_steps)\n",
    "    diffusion_instance = diffusion.MaskDiffusion(\n",
    "        dim=tokenizer.vocab_size,\n",
    "        schedule=diffusion_schedule,\n",
    "        tokenizer=tokenizer,\n",
    "        sample_cls=sample_cls,\n",
    "        word_freq=word_freq,\n",
    "        word_freq_lambda=word_freq_lambda,\n",
    "        device=device\n",
    "    )\n",
    "elif kind == 'base':\n",
    "    import diffusion\n",
    "    diffusion_schedule = diffusion.create_discrete_diffusion_schedule(schedule, num_steps=num_steps)\n",
    "    diffusion_instance = diffusion.MaskDiffusion(\n",
    "        dim=tokenizer.vocab_size,\n",
    "        schedule=diffusion_schedule,\n",
    "        tokenizer=tokenizer,\n",
    "        sample_cls=sample_cls,\n",
    "    )\n",
    "else:\n",
    "    raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "cfg = cfg_cls.from_pretrained(model_name)\n",
    "cfg.overall_timestep = diffusion_instance.num_steps\n",
    "\n",
    "if model_name in ['fnlp/elasticbert-base', 'fnlp/elasticbert-large']:\n",
    "    cfg.num_output_layers = cfg.num_hidden_layers\n",
    "    cfg.num_base_layers = 0\n",
    "\n",
    "model = model_cls(cfg).to(device)\n",
    "ckpt = torch.load(model_ckpt_path)\n",
    "model.load_state_dict(ckpt['model'])\n",
    "\n",
    "cls = torch.full((1, 1), fill_value=tokenizer.cls_token_id, device=device)\n",
    "sep = torch.full((1, 1), fill_value=tokenizer.sep_token_id, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denoise function setup\n",
    "att_ones = torch.ones((1, 1), device=device)\n",
    "att_zeros = torch.zeros((1, 1), device=device)\n",
    "\n",
    "def denoise_fn(targets, timestep, attention_mask):\n",
    "    assert len(targets.size()) == 2  # bsz * seqlen\n",
    "    bsz = targets.size(0)\n",
    "    targets = torch.cat((cls.repeat(bsz, 1), targets, sep.repeat(bsz, 1)), dim=1)\n",
    "    attention_mask = torch.cat((att_ones.repeat(bsz, 1), attention_mask, att_zeros.repeat(bsz, 1)), dim=1)\n",
    "    return model(\n",
    "        input_ids=targets,\n",
    "        attention_mask=attention_mask\n",
    "    )['logits'][:, 1:-1, :]\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing functions\n",
    "def process_fn_in_collate(wf):\n",
    "    return wf - wf.mean()\n",
    "\n",
    "def collate_fn(batch_input):\n",
    "    input_ids = [torch.tensor(d['input_ids']) for d in batch_input]\n",
    "    attention_mask = [torch.tensor(d['attention_mask']) for d in batch_input]\n",
    "    word_freq_logits = [process_fn_in_collate(word_freq.gather(0, torch.tensor(d['input_ids']))) for d in batch_input]\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True)\n",
    "    attention_mask = pad_sequence(attention_mask, batch_first=True)\n",
    "    word_freq_logits = pad_sequence(word_freq_logits, batch_first=True)\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'word_freq_logits': word_freq_logits\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "elbo = 0.\n",
    "count = 0\n",
    "\n",
    "test_data = DiffusionLoader(tokenizer=tokenizer).my_load(task_name='lm1b', splits=['test'])[0]\n",
    "_, test_data = test_data.train_test_split(test_size=5e-2).values()\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=64, collate_fn=collate_fn, num_workers=4, pin_memory=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        batch_dev_metrics = diffusion.discrete_diffusion_elbo(\n",
    "            batch['input_ids'].to(device),\n",
    "            denoise_fn=denoise_fn,\n",
    "            diffusion=diffusion_instance,\n",
    "            target_mask=batch['attention_mask'].to(device),\n",
    "            word_freq_logits=batch['word_freq_logits'].to(device),\n",
    "            normalize_without_padding=True,\n",
    "            eval_step_size=eval_step_size,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        if not torch.isnan(batch_dev_metrics['elbo']):\n",
    "            logger.info(batch_dev_metrics['elbo'])\n",
    "            elbo += batch_dev_metrics['elbo']\n",
    "            count += 1\n",
    "\n",
    "print(f\"Final ELBO: {elbo / (64. * count)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
