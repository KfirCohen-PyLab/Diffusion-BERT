{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DiffusionBERT Training\n",
    "\n",
    "This notebook implements the training pipeline for DiffusionBERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install transformers datasets torch tqdm accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "    level=logging.INFO\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class TrainingConfig:\n",
    "    def __init__(self):\n",
    "        # Model settings\n",
    "        self.model_name = \"bert-base-uncased\"\n",
    "        self.max_seq_length = 128\n",
    "        self.vocab_size = 30522\n",
    "        \n",
    "        # Training settings\n",
    "        self.batch_size = 32\n",
    "        self.learning_rate = 1e-4\n",
    "        self.num_train_epochs = 3\n",
    "        self.warmup_steps = 10000\n",
    "        self.gradient_accumulation_steps = 2\n",
    "        self.fp16 = True\n",
    "        self.seed = 42\n",
    "        \n",
    "        # Diffusion settings\n",
    "        self.diffusion_steps = 2000\n",
    "        self.noise_schedule = \"cosine\"\n",
    "        self.word_freq_lambda = 0.3\n",
    "        \n",
    "        # Paths\n",
    "        self.output_dir = \"./checkpoints\"\n",
    "        self.word_freq_path = \"./word_freq/word_freq.pt\"\n",
    "        self.train_data_dir = \"./data\"\n",
    "        \n",
    "        # Device\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"Set random seeds for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def setup_model_and_tokenizer(config):\n",
    "    \"\"\"Initialize model and tokenizer\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Loading tokenizer from {config.model_name}\")\n",
    "        tokenizer = BertTokenizer.from_pretrained(config.model_name)\n",
    "        \n",
    "        logger.info(\"Initializing model configuration\")\n",
    "        model_config = BertConfig.from_pretrained(config.model_name)\n",
    "        model_config.vocab_size = config.vocab_size\n",
    "        \n",
    "        logger.info(\"Initializing model\")\n",
    "        from models.modeling_diffusion_bert import DiffusionBertForMaskedLM\n",
    "        model = DiffusionBertForMaskedLM(model_config)\n",
    "        model = model.to(config.device)\n",
    "        \n",
    "        return model, tokenizer\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error setting up model and tokenizer: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def setup_data_loader(config, tokenizer):\n",
    "    \"\"\"Setup data loading pipeline\"\"\"\n",
    "    try:\n",
    "        from dataloader import DiffusionLoader\n",
    "        loader = DiffusionLoader(tokenizer)\n",
    "        \n",
    "        logger.info(\"Loading datasets\")\n",
    "        train_data, dev_data = loader.my_load(\"lm1b\", splits=[\"train\", \"validation\"])\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_data,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        dev_loader = torch.utils.data.DataLoader(\n",
    "            dev_data,\n",
    "            batch_size=config.batch_size * 2,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        return train_loader, dev_loader\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error setting up data loaders: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def train_epoch(model, train_loader, optimizer, scheduler, config, epoch):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    step = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        try:\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(config.device)\n",
    "            attention_mask = batch['attention_mask'].to(config.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=input_ids\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss / config.gradient_accumulation_steps\n",
    "            \n",
    "            # Backward pass\n",
    "            if config.fp16:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Update weights\n",
    "            if (step + 1) % config.gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': total_loss / (step + 1),\n",
    "                'lr': scheduler.get_last_lr()[0]\n",
    "            })\n",
    "            \n",
    "            step += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in training step: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return total_loss / step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def evaluate(model, dev_loader, config):\n",
    "    \"\"\"Evaluate the model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_steps = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dev_loader, desc=\"Evaluating\"):\n",
    "            try:\n",
    "                # Move batch to device\n",
    "                input_ids = batch['input_ids'].to(config.device)\n",
    "                attention_mask = batch['attention_mask'].to(config.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=input_ids\n",
    "                )\n",
    "                \n",
    "                total_loss += outputs.loss.item()\n",
    "                total_steps += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in evaluation step: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    return total_loss / total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def save_checkpoint(model, optimizer, scheduler, epoch, loss, config):\n",
    "    \"\"\"Save model checkpoint\"\"\"\n",
    "    try:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': loss,\n",
    "            'config': vars(config)\n",
    "        }\n",
    "        \n",
    "        path = os.path.join(config.output_dir, f'checkpoint-epoch-{epoch+1}.pt')\n",
    "        torch.save(checkpoint, path)\n",
    "        logger.info(f\"Checkpoint saved to {path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving checkpoint: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def main():\n",
    "    try:\n",
    "        # Set random seed\n",
    "        set_seed(config.seed)\n",
    "        \n",
    "        # Setup model and tokenizer\n",
    "        model, tokenizer = setup_model_and_tokenizer(config)\n",
    "        \n",
    "        # Setup data loaders\n",
    "        train_loader, dev_loader = setup_data_loader(config, tokenizer)\n",
    "        \n",
    "        # Setup optimizer and scheduler\n",
    "        optimizer = AdamW(model.parameters(), lr=config.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "            optimizer,\n",
    "            total_iters=config.warmup_steps\n",
    "        )\n",
    "        \n",
    "        # Setup mixed precision training if enabled\n",
    "        if config.fp16:\n",
    "            scaler = torch.cuda.amp.GradScaler()\n",
    "        \n",
    "        # Training loop\n",
    "        best_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(config.num_train_epochs):\n",
    "            # Train\n",
    "            train_loss = train_epoch(model, train_loader, optimizer, scheduler, config, epoch)\n",
    "            logger.info(f\"Epoch {epoch+1} - Training Loss: {train_loss:.4f}\")\n",
    "            \n",
    "            # Evaluate\n",
    "            eval_loss = evaluate(model, dev_loader, config)\n",
    "            logger.info(f\"Epoch {epoch+1} - Evaluation Loss: {eval_loss:.4f}\")\n",
    "            \n",
    "            # Save checkpoint if best model\n",
    "            if eval_loss < best_loss:\n",
    "                best_loss = eval_loss\n",
    "                save_checkpoint(model, optimizer, scheduler, epoch, eval_loss, config)\n",
    "                logger.info(f\"New best model saved with loss: {best_loss:.4f}\")\n",
    "            \n",
    "            # Save regular checkpoint\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                save_checkpoint(model, optimizer, scheduler, epoch, eval_loss, config)\n",
    "        \n",
    "        logger.info(\"Training completed!\")\n",
    "        return model, tokenizer, best_loss\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main execution: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model, tokenizer, best_loss = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "After training, let's evaluate the model's performance on some test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_best_model(config):\n",
    "    \"\"\"Load the best checkpoint\"\"\"\n",
    "    try:\n",
    "        # Find the best checkpoint\n",
    "        checkpoints = [f for f in os.listdir(config.output_dir) if f.endswith('.pt')]\n",
    "        if not checkpoints:\n",
    "            raise ValueError(\"No checkpoints found!\")\n",
    "            \n",
    "        best_checkpoint = None\n",
    "        best_loss = float('inf')\n",
    "        \n",
    "        for checkpoint_file in checkpoints:\n",
    "            checkpoint = torch.load(os.path.join(config.output_dir, checkpoint_file))\n",
    "            if checkpoint['loss'] < best_loss:\n",
    "                best_loss = checkpoint['loss']\n",
    "                best_checkpoint = checkpoint\n",
    "        \n",
    "        # Load model with best checkpoint\n",
    "        model_config = BertConfig.from_pretrained(config.model_name)\n",
    "        model_config.vocab_size = config.vocab_size\n",
    "        \n",
    "        from models.modeling_diffusion_bert import DiffusionBertForMaskedLM\n",
    "        model = DiffusionBertForMaskedLM(model_config)\n",
    "        model.load_state_dict(best_checkpoint['model_state_dict'])\n",
    "        model = model.to(config.device)\n",
    "        model.eval()\n",
    "        \n",
    "        return model, best_checkpoint['epoch'], best_checkpoint['loss']\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading best model: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the best model\n",
    "best_model, best_epoch, best_loss = load_best_model(config)\n",
    "print(f\"Loaded best model from epoch {best_epoch+1} with loss {best_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def evaluate_on_examples(model, tokenizer, examples):\n",
    "    \"\"\"Evaluate model on specific examples\"\"\"\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text in examples:\n",
    "            # Tokenize input\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Get model predictions\n",
    "            outputs = model(**inputs)\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            \n",
    "            # Decode predictions\n",
    "            predicted_text = tokenizer.decode(predictions[0], skip_special_tokens=True)\n",
    "            \n",
    "            results.append({\n",
    "                'input': text,\n",
    "                'output': predicted_text\n",
    "            })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test examples\n",
    "test_examples = [\n",
    "    \"The quick brown fox jumps over the lazy [MASK].\",\n",
    "    \"I love to [MASK] in my free time.\",\n",
    "    \"The capital of France is [MASK].\"\n",
    "]\n",
    "\n",
    "results = evaluate_on_examples(best_model, tokenizer, test_examples)\n",
    "\n",
    "print(\"\\nEvaluation Results:\")\n",
    "for result in results:\n",
    "    print(f\"\\nInput: {result['input']}\")\n",
    "    print(f\"Output: {result['output']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save evaluation results\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "eval_results = {\n",
    "    'timestamp': timestamp,\n",
    "    'best_epoch': best_epoch,\n",
    "    'best_loss': best_loss,\n",
    "    'examples': results\n",
    "}\n",
    "\n",
    "eval_file = os.path.join(config.output_dir, f'eval_results_{timestamp}.json')\n",
    "with open(eval_file, 'w') as f:\n",
    "    json.dump(eval_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nEvaluation results saved to {eval_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}