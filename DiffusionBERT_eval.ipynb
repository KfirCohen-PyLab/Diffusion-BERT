{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5mEwserLBLF"
      },
      "source": [
        "# DiffusionBERT Model Evaluation\n",
        "\n",
        "This notebook provides a comprehensive evaluation of the DiffusionBERT model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XovG8PJVLBLH"
      },
      "source": [
        "## 0.1 Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gprySNaALBLH",
        "outputId": "62b26667-1334-4220-96b2-b1ffa6c6f13c"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Unmount (if previously mounted) and then mount the Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set working directory to your Drive folder\n",
        "%cd /content/drive/MyDrive/DiffusionBERT   # Update this path if necessary"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/DiffusionBERT # Update this path if necessary'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqdqxWu7LBLI"
      },
      "source": [
        "## 0.2 Clone Repository and Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2-CUyMpLBLI",
        "outputId": "d517e3ba-2894-43fd-d437-2eff00c2ad2d"
      },
      "source": [
        "# Clone the repository if it doesn't exist\n",
        "!git clone https://github.com/KfirCohen-PyLab/Diffusion-BERT.git\n",
        "%cd Diffusion-BERT\n",
        "\n",
        "# Install compatible versions\n",
        "!pip install -q transformers fastNLP nltk\n",
        "!pip install -q torch==2.0.1\n",
        "!pip install -q numpy==1.24.3\n",
        "!pip install -q pandas==2.2.2\n",
        "!pip install -q matplotlib==3.7.1\n",
        "!pip install -q seaborn==0.12.2\n",
        "!pip install -q tqdm==4.65.0\n",
        "!pip install -q tensorboard==2.13.0\n",
        "\n",
        "# Reinstall the exact same NumPy version to prevent dependency breakage\n",
        "!pip install --upgrade --force-reinstall numpy==1.26\n",
        "\n",
        "\n",
        "# Verify installation\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Diffusion-BERT'...\n",
            "remote: Enumerating objects: 7129, done.\u001b[K\n",
            "remote: Counting objects: 100% (72/72), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 7129 (delta 49), reused 58 (delta 42), pack-reused 7057 (from 3)\u001b[K\n",
            "Receiving objects: 100% (7129/7129), 48.41 MiB | 16.84 MiB/s, done.\n",
            "Resolving deltas: 100% (511/511), done.\n",
            "Updating files: 100% (6462/6462), done.\n",
            "/content/Diffusion-BERT/Diffusion-BERT\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m126.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 2.4.0 requires rich<14,>=12.4.4, but you have rich 11.2.0 which is incompatible.\n",
            "pymc 5.22.0 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\n",
            "pymc 5.22.0 requires rich>=13.7.1, but you have rich 11.2.0 which is incompatible.\n",
            "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting numpy==1.26\n",
            "  Downloading numpy-1.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.24.3\n",
            "    Uninstalling numpy-1.24.3:\n",
            "      Successfully uninstalled numpy-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.18.0 requires tensorboard<2.19,>=2.18, but you have tensorboard 2.13.0 which is incompatible.\n",
            "bigframes 2.4.0 requires rich<14,>=12.4.4, but you have rich 11.2.0 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.0.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.0 which is incompatible.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.30.2 which is incompatible.\n",
            "pymc 5.22.0 requires rich>=13.7.1, but you have rich 11.2.0 which is incompatible.\n",
            "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.0\n",
            "PyTorch version: 2.0.1+cu117\n",
            "CUDA available: True\n",
            "CUDA version: 11.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVNG0bOlLBLI"
      },
      "source": [
        "## 0.3 Setup File Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgZW40LALBLI",
        "outputId": "036a16f5-c8a2-4e05-d0db-0559b5a4aa18"
      },
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Create necessary directories\n",
        "!mkdir -p word_freq\n",
        "!mkdir -p checkpoints\n",
        "!mkdir -p evaluation_results\n",
        "\n",
        "# Copy files from Drive\n",
        "drive_paths = {\n",
        "    'checkpoint': '/content/drive/MyDrive/DiffusionBERT/diffusion_bert_lm1b_final.pt',\n",
        "    'word_freq': '/content/drive/MyDrive/DiffusionBERT/word_freq.pt',\n",
        "    'config': '/content/drive/MyDrive/DiffusionBERT/word_freq.json'\n",
        "}\n",
        "\n",
        "local_paths = {\n",
        "    'checkpoint': 'diffusion_bert_lm1b_final.pt',\n",
        "    'word_freq': 'word_freq.pt',\n",
        "    'config': 'word_freq.json',\n",
        "}\n",
        "\n",
        "for key in drive_paths:\n",
        "    if os.path.exists(drive_paths[key]):\n",
        "        shutil.copy2(drive_paths[key], local_paths[key])\n",
        "        print(f\"Copied {key} file successfully\")\n",
        "    else:\n",
        "        print(f\"Warning: {key} file not found in Drive\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copied checkpoint file successfully\n",
            "Copied word_freq file successfully\n",
            "Copied config file successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1MezA7WLBLI"
      },
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKDulZFYLBLJ",
        "outputId": "4da19639-a35b-4294-fe72-810620aacf6e"
      },
      "source": [
        "# -- Imports --\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import json\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "try:\n",
        "    from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
        "except ImportError:\n",
        "    !pip install transformers\n",
        "    from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
        "\n",
        "# If your custom model is in a local file, make sure the path is correct\n",
        "# e.g., /content/drive/MyDrive/DiffusionBERT/models/modeling_diffusion_bert.py\n",
        "try:\n",
        "    from models.modeling_diffusion_bert import DiffusionBertForMaskedLM\n",
        "except ImportError as e:\n",
        "    raise ImportError(\n",
        "        \"Could not import DiffusionBertForMaskedLM. Make sure your models/ directory \"\n",
        "        \"is in the current working directory and contains modeling_diffusion_bert.py.\"\n",
        "    ) from e\n",
        "\n",
        "# -- Logging setup --\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# -- Set seeds --\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# -- Device setup --\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n",
            "  Using cached numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Using cached numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.0\n",
            "    Uninstalling numpy-1.26.0:\n",
            "      Successfully uninstalled numpy-1.26.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\n",
            "tensorflow 2.18.0 requires tensorboard<2.19,>=2.18, but you have tensorboard 2.13.0 which is incompatible.\n",
            "bigframes 2.4.0 requires rich<14,>=12.4.4, but you have rich 11.2.0 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.0.1 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.30.2 which is incompatible.\n",
            "pymc 5.22.0 requires rich>=13.7.1, but you have rich 11.2.0 which is incompatible.\n",
            "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/_core/_dtype.py:106: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
            "  if dtype.type == np.bool:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvqB2m-TLBLJ"
      },
      "source": [
        "## 2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rOq0zs7_LBLJ"
      },
      "source": [
        "config = {\n",
        "    'model_name': 'bert-base-uncased',\n",
        "    'model_checkpoint_path': 'diffusion_bert_lm1b_final.pt',\n",
        "    'word_freq_path': 'word_freq.pt',\n",
        "    'output_dir': 'evaluation_results',\n",
        "    'max_position_embeddings': 512,\n",
        "    'max_seq_length': 128,\n",
        "    'batch_size': 32,\n",
        "    'num_samples': 1000,\n",
        "    'temperature': 1.0,\n",
        "    'top_k': 50,\n",
        "    'top_p': 0.9\n",
        "}"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsc_xBXnLBLJ"
      },
      "source": [
        "## 3. Model Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "eCoejNY4LBLJ",
        "outputId": "86b3b517-697a-4110-b63e-2e0c98201196"
      },
      "source": [
        "def load_model_and_tokenizer(config):\n",
        "    try:\n",
        "        # Register custom model\n",
        "        AutoConfig.register(\"diffusion-bert\", DiffusionBertForMaskedLM)\n",
        "        AutoModel.register(DiffusionBertForMaskedLM, \"diffusion-bert\")\n",
        "\n",
        "        # Load tokenizer and config\n",
        "        tokenizer = AutoTokenizer.from_pretrained(config['model_name'])\n",
        "        model_config = AutoConfig.from_pretrained(config['model_name'])\n",
        "        model_config.max_position_embeddings = config['max_position_embeddings']\n",
        "\n",
        "        # Initialize and load model\n",
        "        model = DiffusionBertForMaskedLM(model_config)\n",
        "        checkpoint = torch.load(config['model_checkpoint_path'], map_location='cpu')\n",
        "        model.load_state_dict(checkpoint['model'])\n",
        "\n",
        "        # Move to device\n",
        "        model = model.to(device)\n",
        "        model.eval()\n",
        "\n",
        "        return model, tokenizer\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading model: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "model, tokenizer = load_model_and_tokenizer(config)\n",
        "print(\"Model loaded successfully\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error loading model: 'model'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'model'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-5c230377591d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model_and_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model loaded successfully\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-5c230377591d>\u001b[0m in \u001b[0;36mload_model_and_tokenizer\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiffusionBertForMaskedLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_checkpoint_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Move to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'model'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUIcDVTwLBLK"
      },
      "source": [
        "## 4. Load Word Frequencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Za3Ruve5LBLK"
      },
      "source": [
        "def load_word_frequencies(config):\n",
        "    try:\n",
        "        word_freq = torch.load(config['word_freq_path'])\n",
        "        word_freq = word_freq + 1  # Add smoothing\n",
        "        word_freq = word_freq.log()\n",
        "        word_freq = word_freq / word_freq.max()\n",
        "        return word_freq.to(device)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading word frequencies: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "word_freq = load_word_frequencies(config)\n",
        "print(f\"Word frequencies loaded with shape: {word_freq.shape}\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xUcLAcGLBLK"
      },
      "source": [
        "## 5. Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2YogRQHLBLK"
      },
      "source": [
        "def evaluate_model(model, tokenizer, word_freq, config):\n",
        "    results = {\n",
        "        'perplexities': [],\n",
        "        'word_freq_scores': [],\n",
        "        'samples': []\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in tqdm(range(config['num_samples']), desc=\"Evaluating\"):\n",
        "            # Generate sample\n",
        "            input_ids = torch.randint(100, 1000, (1, config['max_seq_length'])).to(device)\n",
        "            attention_mask = torch.ones_like(input_ids)\n",
        "\n",
        "            outputs = model.generate(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                max_length=config['max_seq_length'],\n",
        "                temperature=config['temperature'],\n",
        "                top_k=config['top_k'],\n",
        "                top_p=config['top_p'],\n",
        "                do_sample=True\n",
        "            )\n",
        "\n",
        "            # Compute metrics\n",
        "            loss = model(input_ids=outputs, labels=outputs).loss\n",
        "            perplexity = torch.exp(loss).item()\n",
        "            word_freq_score = word_freq.gather(0, outputs.view(-1)).mean().item()\n",
        "\n",
        "            # Store results\n",
        "            results['perplexities'].append(perplexity)\n",
        "            results['word_freq_scores'].append(word_freq_score)\n",
        "            results['samples'].append({\n",
        "                'input': tokenizer.decode(input_ids[0]),\n",
        "                'generated': tokenizer.decode(outputs[0]),\n",
        "                'perplexity': perplexity,\n",
        "                'word_freq_score': word_freq_score\n",
        "            })\n",
        "\n",
        "    return results"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lK-ScenxLBLK"
      },
      "source": [
        "## 6. Run Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "4auRQBv6LBLK",
        "outputId": "f4dcfbcd-3b7d-4298-f9aa-28b3b11102c6"
      },
      "source": [
        "print(\"Starting evaluation...\")\n",
        "results = evaluate_model(model, tokenizer, word_freq, config)\n",
        "\n",
        "# Compute metrics\n",
        "metrics = {\n",
        "    'avg_perplexity': np.mean(results['perplexities']),\n",
        "    'std_perplexity': np.std(results['perplexities']),\n",
        "    'avg_word_freq_score': np.mean(results['word_freq_scores']),\n",
        "    'std_word_freq_score': np.std(results['word_freq_scores'])\n",
        "}\n",
        "\n",
        "results['metrics'] = metrics\n",
        "print(\"\\nEvaluation Results:\")\n",
        "print(f\"Average Perplexity: {metrics['avg_perplexity']:.2f} ± {metrics['std_perplexity']:.2f}\")\n",
        "print(f\"Word Freq Score: {metrics['avg_word_freq_score']:.4f} ± {metrics['std_word_freq_score']:.4f}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting evaluation...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'evaluate_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-c08360d5b1bc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting evaluation...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Compute metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m metrics = {\n",
            "\u001b[0;31mNameError\u001b[0m: name 'evaluate_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAWR0i8kLBLK"
      },
      "source": [
        "## 7. Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6RA_vj2LBLL"
      },
      "source": [
        "# Save results\n",
        "output_dir = Path(config['output_dir'])\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
        "output_file = output_dir / f\"eval_results_{timestamp}.json\"\n",
        "\n",
        "with open(output_file, 'w') as f:\n",
        "    json.dump({\n",
        "        'config': config,\n",
        "        'metrics': metrics,\n",
        "        'samples': results['samples'][:10]  # Save first 10 samples\n",
        "    }, f, indent=2)\n",
        "\n",
        "print(f\"Results saved to {output_file}\")"
      ],
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}