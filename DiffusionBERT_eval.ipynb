{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DiffusionBERT Evaluation\n",
    "\n",
    "This notebook is for evaluating pre-trained DiffusionBERT models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install transformers datasets torch tqdm accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import logging\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "    level=logging.INFO\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.model_name = \"bert-base-uncased\"\n",
    "        self.checkpoint_path = \"/content/drive/MyDrive/DiffusionBERT/checkpoints/diffusion_bert_lm1b_final.pt\"\n",
    "        self.word_freq_path = \"/content/drive/MyDrive/DiffusionBERT/word_freqs/word_freq.pt\"\n",
    "        self.output_dir = \"/content/drive/MyDrive/DiffusionBERT/evaluation_results\"\n",
    "        self.batch_size = 32\n",
    "        self.max_seq_length = 128\n",
    "        self.num_eval_samples = 1000\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_model_and_tokenizer(config):\n",
    "    \"\"\"Load the pre-trained model and tokenizer\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Loading tokenizer from {config.model_name}\")\n",
    "        tokenizer = BertTokenizer.from_pretrained(config.model_name)\n",
    "        \n",
    "        logger.info(f\"Loading model from {config.checkpoint_path}\")\n",
    "        checkpoint = torch.load(config.checkpoint_path, map_location=config.device)\n",
    "        \n",
    "        # Load model configuration\n",
    "        model_config = BertConfig.from_pretrained(config.model_name)\n",
    "        model_config.vocab_size = tokenizer.vocab_size\n",
    "        \n",
    "        # Initialize model with config\n",
    "        from models.modeling_bert import BertForMaskedLM\n",
    "        model = BertForMaskedLM(model_config)\n",
    "        \n",
    "        # Load checkpoint weights\n",
    "        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint)\n",
    "            \n",
    "        model = model.to(config.device)\n",
    "        model.eval()\n",
    "        \n",
    "        return model, tokenizer\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading model and tokenizer: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_word_frequencies(config):\n",
    "    \"\"\"Load word frequencies\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Loading word frequencies from {config.word_freq_path}\")\n",
    "        word_freq = torch.load(config.word_freq_path, map_location=config.device)\n",
    "        \n",
    "        # Preprocess word frequencies\n",
    "        word_freq = word_freq + 1  # Add smoothing\n",
    "        word_freq = word_freq.log()\n",
    "        word_freq = word_freq / word_freq.max()\n",
    "        \n",
    "        return word_freq\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading word frequencies: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def evaluate_model(model, tokenizer, word_freq, config):\n",
    "    \"\"\"Evaluate the model on various metrics\"\"\"\n",
    "    try:\n",
    "        from dataloader import DiffusionLoader\n",
    "        loader = DiffusionLoader(tokenizer)\n",
    "        eval_data = loader.my_load(\"lm1b\", splits=[\"validation\"])[0]\n",
    "        \n",
    "        results = {\n",
    "            'perplexity': [],\n",
    "            'elbo': [],\n",
    "            'word_freq_score': [],\n",
    "            'generation_samples': []\n",
    "        }\n",
    "        \n",
    "        # Evaluation loop\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(tqdm(eval_data, desc=\"Evaluating\")):\n",
    "                if i >= config.num_eval_samples:\n",
    "                    break\n",
    "                    \n",
    "                input_ids = torch.tensor(batch['input_ids']).unsqueeze(0).to(config.device)\n",
    "                attention_mask = torch.tensor(batch['attention_mask']).unsqueeze(0).to(config.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                # Calculate metrics\n",
    "                loss = torch.nn.functional.cross_entropy(logits.view(-1, tokenizer.vocab_size), \n",
    "                                                        input_ids.view(-1))\n",
    "                perplexity = torch.exp(loss)\n",
    "                \n",
    "                # Word frequency score\n",
    "                pred_tokens = torch.argmax(logits, dim=-1)\n",
    "                freq_score = word_freq[pred_tokens].mean()\n",
    "                \n",
    "                # Store results\n",
    "                results['perplexity'].append(perplexity.item())\n",
    "                results['word_freq_score'].append(freq_score.item())\n",
    "                \n",
    "                # Generate sample text\n",
    "                if i < 10:  # Store first 10 samples\n",
    "                    input_text = tokenizer.decode(input_ids[0])\n",
    "                    generated_text = tokenizer.decode(pred_tokens[0])\n",
    "                    results['generation_samples'].append({\n",
    "                        'input': input_text,\n",
    "                        'generated': generated_text\n",
    "                    })\n",
    "        \n",
    "        # Calculate final metrics\n",
    "        final_results = {\n",
    "            'avg_perplexity': np.mean(results['perplexity']),\n",
    "            'std_perplexity': np.std(results['perplexity']),\n",
    "            'avg_word_freq_score': np.mean(results['word_freq_score']),\n",
    "            'std_word_freq_score': np.std(results['word_freq_score']),\n",
    "            'samples': results['generation_samples']\n",
    "        }\n",
    "        \n",
    "        return final_results\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during evaluation: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def save_results(results, config):\n",
    "    \"\"\"Save evaluation results\"\"\"\n",
    "    try:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_file = os.path.join(config.output_dir, f\"eval_results_{timestamp}.json\")\n",
    "        \n",
    "        # Add configuration to results\n",
    "        results['config'] = {\n",
    "            'model_name': config.model_name,\n",
    "            'checkpoint_path': config.checkpoint_path,\n",
    "            'batch_size': config.batch_size,\n",
    "            'max_seq_length': config.max_seq_length,\n",
    "            'num_eval_samples': config.num_eval_samples\n",
    "        }\n",
    "        \n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "            \n",
    "        logger.info(f\"Results saved to {output_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving results: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def main():\n",
    "    try:\n",
    "        # Load model and tokenizer\n",
    "        model, tokenizer = load_model_and_tokenizer(config)\n",
    "        \n",
    "        # Load word frequencies\n",
    "        word_freq = load_word_frequencies(config)\n",
    "        \n",
    "        # Evaluate model\n",
    "        results = evaluate_model(model, tokenizer, word_freq, config)\n",
    "        \n",
    "        # Save results\n",
    "        save_results(results, config)\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\nEvaluation Results:\")\n",
    "        print(f\"Average Perplexity: {results['avg_perplexity']:.2f} ± {results['std_perplexity']:.2f}\")\n",
    "        print(f\"Average Word Frequency Score: {results['avg_word_freq_score']:.4f} ± {results['std_word_freq_score']:.4f}\")\n",
    "        \n",
    "        print(\"\\nSample Generations:\")\n",
    "        for i, sample in enumerate(results['samples'][:3]):\n",
    "            print(f\"\\nSample {i+1}:\")\n",
    "            print(f\"Input: {sample['input']}\")\n",
    "            print(f\"Generated: {sample['generated']}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main execution: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}