{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DiffusionBERT Model Evaluation\n",
    "\n",
    "This notebook provides a comprehensive evaluation of the DiffusionBERT model. It includes:\n",
    "1. Model loading and setup\n",
    "2. Data preparation\n",
    "3. Evaluation metrics computation\n",
    "4. Results visualization\n",
    "5. Sample generation analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "from models.modeling_diffusion_bert import DiffusionBertForMaskedLM\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Enable CUDA if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "config = {\n",
    "    # Model settings\n",
    "    'model_name': 'bert-base-uncased',\n",
    "    'model_checkpoint_path': 'path/to/your/checkpoint.th',  # Update this\n",
    "    'max_position_embeddings': 512,\n",
    "    \n",
    "    # Data settings\n",
    "    'word_freq_path': 'word_freq/bert-base-uncased_lm1b.pt',\n",
    "    'test_data_path': 'data/test.txt',  # Update this\n",
    "    'max_seq_length': 128,\n",
    "    'batch_size': 32,\n",
    "    \n",
    "    # Evaluation settings\n",
    "    'num_samples': 1000,\n",
    "    'temperature': 1.0,\n",
    "    'top_k': 50,\n",
    "    'top_p': 0.9,\n",
    "    \n",
    "    # Output settings\n",
    "    'output_dir': 'evaluation_results',\n",
    "    'save_samples': True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Loading and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_model_and_tokenizer(config):\n",
    "    \"\"\"Load and setup the DiffusionBERT model and tokenizer.\"\"\"\n",
    "    try:\n",
    "        # Register custom model\n",
    "        AutoConfig.register(\"diffusion-bert\", DiffusionBertForMaskedLM)\n",
    "        AutoModel.register(DiffusionBertForMaskedLM, \"diffusion-bert\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(config['model_name'])\n",
    "        \n",
    "        # Load config and update parameters\n",
    "        model_config = AutoConfig.from_pretrained(config['model_name'])\n",
    "        model_config.max_position_embeddings = config['max_position_embeddings']\n",
    "        \n",
    "        # Initialize model\n",
    "        model = DiffusionBertForMaskedLM(model_config)\n",
    "        \n",
    "        # Load checkpoint\n",
    "        checkpoint = torch.load(config['model_checkpoint_path'], map_location='cpu')\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        \n",
    "        # Move to device and set to eval mode\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        return model, tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading model and tokenizer: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = load_model_and_tokenizer(config)\n",
    "print(\"Model and tokenizer loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_word_frequencies(config):\n",
    "    \"\"\"Load and preprocess word frequencies.\"\"\"\n",
    "    try:\n",
    "        word_freq_path = Path(config['word_freq_path'])\n",
    "        if word_freq_path.suffix == '.pt':\n",
    "            word_freq = torch.load(word_freq_path)\n",
    "        else:\n",
    "            with open(word_freq_path) as f:\n",
    "                word_freq = json.load(f)\n",
    "            word_freq = torch.tensor(word_freq)\n",
    "        \n",
    "        # Normalize frequencies\n",
    "        word_freq = word_freq + 1  # Add smoothing\n",
    "        word_freq = word_freq.log()\n",
    "        word_freq = word_freq / word_freq.max()\n",
    "        \n",
    "        return word_freq.to(device)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading word frequencies: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Load word frequencies\n",
    "word_freq = load_word_frequencies(config)\n",
    "print(f\"Word frequencies loaded with shape: {word_freq.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def compute_perplexity(model, input_ids, attention_mask):\n",
    "    \"\"\"Compute perplexity for given input.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "        return torch.exp(outputs.loss).item()\n",
    "\n",
    "def compute_word_freq_score(generated_ids, word_freq):\n",
    "    \"\"\"Compute word frequency score for generated text.\"\"\"\n",
    "    scores = word_freq.gather(0, generated_ids.view(-1))\n",
    "    return scores.mean().item()\n",
    "\n",
    "def evaluate_samples(model, tokenizer, word_freq, config):\n",
    "    \"\"\"Generate and evaluate samples.\"\"\"\n",
    "    results = {\n",
    "        'perplexities': [],\n",
    "        'word_freq_scores': [],\n",
    "        'samples': []\n",
    "    }\n",
    "    \n",
    "    for _ in tqdm(range(config['num_samples']), desc=\"Generating samples\"):\n",
    "        # Generate sample\n",
    "        input_ids = torch.randint(100, 1000, (1, config['max_seq_length'])).to(device)\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=config['max_seq_length'],\n",
    "                temperature=config['temperature'],\n",
    "                top_k=config['top_k'],\n",
    "                top_p=config['top_p'],\n",
    "                do_sample=True\n",
    "            )\n",
    "        \n",
    "        # Compute metrics\n",
    "        perplexity = compute_perplexity(model, outputs, attention_mask)\n",
    "        word_freq_score = compute_word_freq_score(outputs, word_freq)\n",
    "        \n",
    "        # Store results\n",
    "        results['perplexities'].append(perplexity)\n",
    "        results['word_freq_scores'].append(word_freq_score)\n",
    "        \n",
    "        if config['save_samples']:\n",
    "            results['samples'].append({\n",
    "                'input': tokenizer.decode(input_ids[0]),\n",
    "                'generated': tokenizer.decode(outputs[0]),\n",
    "                'perplexity': perplexity,\n",
    "                'word_freq_score': word_freq_score\n",
    "            })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run evaluation\n",
    "print(\"Starting evaluation...\")\n",
    "evaluation_results = evaluate_samples(model, tokenizer, word_freq, config)\n",
    "\n",
    "# Compute summary statistics\n",
    "metrics = {\n",
    "    'avg_perplexity': np.mean(evaluation_results['perplexities']),\n",
    "    'std_perplexity': np.std(evaluation_results['perplexities']),\n",
    "    'avg_word_freq_score': np.mean(evaluation_results['word_freq_scores']),\n",
    "    'std_word_freq_score': np.std(evaluation_results['word_freq_scores'])\n",
    "}\n",
    "\n",
    "# Add metrics to results\n",
    "evaluation_results['metrics'] = metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set up the visualization style\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot perplexity distribution\n",
    "sns.histplot(evaluation_results['perplexities'], ax=ax1, kde=True)\n",
    "ax1.set_title('Perplexity Distribution')\n",
    "ax1.set_xlabel('Perplexity')\n",
    "ax1.axvline(metrics['avg_perplexity'], color='r', linestyle='--', \n",
    "            label=f'Mean: {metrics[\"avg_perplexity\"]:.2f}')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot word frequency score distribution\n",
    "sns.histplot(evaluation_results['word_freq_scores'], ax=ax2, kde=True)\n",
    "ax2.set_title('Word Frequency Score Distribution')\n",
    "ax2.set_xlabel('Word Frequency Score')\n",
    "ax2.axvline(metrics['avg_word_freq_score'], color='r', linestyle='--',\n",
    "            label=f'Mean: {metrics[\"avg_word_freq_score\"]:.4f}')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sample Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display best samples (lowest perplexity)\n",
    "print(\"Best Samples (Lowest Perplexity):\")\n",
    "best_samples = sorted(evaluation_results['samples'], key=lambda x: x['perplexity'])[:3]\n",
    "for i, sample in enumerate(best_samples, 1):\n",
    "    print(f\"\\nSample {i}:\")\n",
    "    print(f\"Generated: {sample['generated']}\")\n",
    "    print(f\"Perplexity: {sample['perplexity']:.2f}\")\n",
    "    print(f\"Word Freq Score: {sample['word_freq_score']:.4f}\")\n",
    "\n",
    "# Display samples with highest word frequency scores\n",
    "print(\"\\nSamples with Highest Word Frequency Scores:\")\n",
    "best_freq_samples = sorted(evaluation_results['samples'], key=lambda x: x['word_freq_score'], reverse=True)[:3]\n",
    "for i, sample in enumerate(best_freq_samples, 1):\n",
    "    print(f\"\\nSample {i}:\")\n",
    "    print(f\"Generated: {sample['generated']}\")\n",
    "    print(f\"Perplexity: {sample['perplexity']:.2f}\")\n",
    "    print(f\"Word Freq Score: {sample['word_freq_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def save_results(results, config):\n",
    "    \"\"\"Save evaluation results to file.\"\"\"\n",
    "    try:\n",
    "        # Create output directory\n",
    "        output_dir = Path(config['output_dir'])\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Add timestamp\n",
    "        results['timestamp'] = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "        results['config'] = config\n",
    "        \n",
    "        # Save results\n",
    "        output_file = output_dir / f\"eval_results_{results['timestamp']}.json\"\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "            \n",
    "        print(f\"Results saved to {output_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving results: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Save results\n",
    "save_results(evaluation_results, config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}