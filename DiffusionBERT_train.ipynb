{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DiffusionBERT Training\n",
    "\n",
    "This notebook implements the training pipeline for DiffusionBERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install transformers datasets torch tqdm accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "    level=logging.INFO\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class TrainingConfig:\n",
    "    def __init__(self):\n",
    "        # Model settings\n",
    "        self.model_name = \"bert-base-uncased\"\n",
    "        self.max_seq_length = 128\n",
    "        self.vocab_size = 30522\n",
    "        \n",
    "        # Training settings\n",
    "        self.batch_size = 32\n",
    "        self.learning_rate = 1e-4\n",
    "        self.num_train_epochs = 3\n",
    "        self.warmup_steps = 10000\n",
    "        self.gradient_accumulation_steps = 2\n",
    "        self.fp16 = True\n",
    "        self.seed = 42\n",
    "        \n",
    "        # Diffusion settings\n",
    "        self.diffusion_steps = 2000\n",
    "        self.noise_schedule = \"cosine\"\n",
    "        self.word_freq_lambda = 0.3\n",
    "        \n",
    "        # Paths\n",
    "        self.output_dir = \"/content/drive/MyDrive/DiffusionBERT/training_output\"\n",
    "        self.word_freq_path = \"/content/drive/MyDrive/DiffusionBERT/word_freqs/word_freq.pt\"\n",
    "        self.train_data_dir = \"/content/drive/MyDrive/DiffusionBERT/data\"\n",
    "        \n",
    "        # Device\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"Set random seeds for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def setup_model_and_tokenizer(config):\n",
    "    \"\"\"Initialize model and tokenizer\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Loading tokenizer from {config.model_name}\")\n",
    "        tokenizer = BertTokenizer.from_pretrained(config.model_name)\n",
    "        \n",
    "        logger.info(\"Initializing model configuration\")\n",
    "        model_config = BertConfig.from_pretrained(config.model_name)\n",
    "        model_config.vocab_size = config.vocab_size\n",
    "        \n",
    "        logger.info(\"Initializing model\")\n",
    "        from models.modeling_bert import BertForMaskedLM\n",
    "        model = BertForMaskedLM(model_config)\n",
    "        model = model.to(config.device)\n",
    "        \n",
    "        return model, tokenizer\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error setting up model and tokenizer: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def setup_data_loader(config, tokenizer):\n",
    "    \"\"\"Setup data loading pipeline\"\"\"\n",
    "    try:\n",
    "        from dataloader import DiffusionLoader\n",
    "        loader = DiffusionLoader(tokenizer)\n",
    "        \n",
    "        logger.info(\"Loading datasets\")\n",
    "        train_data, dev_data = loader.my_load(\"lm1b\", splits=[\"train\", \"validation\"])\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_data,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        dev_loader = torch.utils.data.DataLoader(\n",
    "            dev_data,\n",
    "            batch_size=config.batch_size * 2,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        return train_loader, dev_loader\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error setting up data loaders: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def train_epoch(model, train_loader, optimizer, scheduler, config, epoch):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    step = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        try:\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(config.device)\n",
    "            attention_mask = batch['attention_mask'].to(config.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=input_ids\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss / config.gradient_accumulation_steps\n",
    "            \n",
    "            # Backward pass\n",
    "            if config.fp16:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Update weights\n",
    "            if (step + 1) % config.gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': total_loss / (step + 1),\n",
    "                'lr': scheduler.get_last_lr()[0]\n",
    "            })\n",
    "            \n",
    "            step += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in training step: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return total_loss / step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def evaluate(model, dev_loader, config):\n",
    "    \"\"\"Evaluate the model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_steps = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dev_loader, desc=\"Evaluating\"):\n",
    "            try:\n",
    "                # Move batch to device\n",
    "                input_ids = batch['input_ids'].to(config.device)\n",
    "                attention_mask = batch['attention_mask'].to(config.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=input_ids\n",
    "                )\n",
    "                \n",
    "                total_loss += outputs.loss.item()\n",
    "                total_steps += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in evaluation step: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    return total_loss / total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def save_checkpoint(model, optimizer, scheduler, epoch, loss, config):\n",
    "    \"\"\"Save model checkpoint\"\"\"\n",
    "    try:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': loss,\n",
    "            'config': vars(config)\n",
    "        }\n",
    "        \n",
    "        path = os.path.join(config.output_dir, f'checkpoint-epoch-{epoch+1}.pt')\n",
    "        torch.save(checkpoint, path)\n",
    "        logger.info(f\"Checkpoint saved to {path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving checkpoint: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def main():\n",
    "    try:\n",
    "        # Set random seed\n",
    "        set_seed(config.seed)\n",
    "        \n",
    "        # Setup model and tokenizer\n",
    "        model, tokenizer = setup_model_and_tokenizer(config)\n",
    "        \n",
    "        # Setup data loaders\n",
    "        train_loader, dev_loader = setup_data_loader(config, tokenizer)\n",
    "        \n",
    "        # Setup optimizer and scheduler\n",
    "        optimizer = AdamW(model.parameters(), lr=config.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "            optimizer,\n",
    "            total_iters=config.warmup_steps\n",
    "        )\n",
    "        \n",
    "        # Setup mixed precision training if enabled\n",
    "        if config.fp16:\n",
    "            scaler = torch.cuda.amp.GradScaler()\n",
    "        \n",
    "        # Training loop\n",
    "        best_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(config.num_train_epochs):\n",
    "            # Train\n",
    "            train_loss = train_epoch(model, train_loader, optimizer, scheduler, config, epoch)\n",
    "            logger.info(f\"Epoch {epoch+1} - Training Loss: {train_loss:.4f}\")\n",
    "            \n",
    "            # Evaluate\n",
    "            eval_loss = evaluate(model, dev_loader, config)\n",
    "            logger.info(f\"Epoch {epoch+1} - Evaluation Loss: {eval_loss:.4f}\")\n",
    "            \n",
    "            # Save checkpoint if best model\n",
    "            if eval_loss < best_loss:\n",
    "                best_loss = eval_loss\n",
    "                save_checkpoint(model, optimizer, scheduler, epoch, eval_loss, config)\n",
    "                logger.info(f\"New best model saved with loss: {best_loss:.4f}\")\n",
    "            \n",
    "            # Save regular checkpoint\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                save_checkpoint(model, optimizer, scheduler, epoch, eval_loss, config)\n",
    "        \n",
    "        logger.info(\"Training completed!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main execution: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}